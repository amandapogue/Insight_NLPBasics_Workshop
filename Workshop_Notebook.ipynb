{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP\n",
    "A short workshop of NLP techniques for those with little or no experience with NLP.\n",
    "\n",
    "## What is NLP?\n",
    "* NLP stands for Natural Language Processing, and the field is concerned with the ability to use computers to manipulate text data. \n",
    "* Researchers in computational linguistics are focused in three areas: NLP, NLG (natural language generation) and NLU (natural language understanding)\n",
    "* NLP researchers tend to study and word on problems that include: \n",
    "  * parsing strings into individual paragraphs, sentences, words, morphemes, etc\n",
    "  * finding grammatical relations and structures\n",
    "  * identifying entities\n",
    "  * comparing strings\n",
    "  * feature extractions, such as: sentiments, topics, etc\n",
    "  \n",
    "Today we're going to cover some of the basics in NLP. Including why and how to preprocess text data\n",
    "\n",
    "First we'll read in some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>Martha's Vineyard</td>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Heitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>Carodorum Selección Especial Reserva</td>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Toro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tinta de Toro</td>\n",
       "      <td>Bodega Carmen Rodríguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>Special Selected Late Harvest</td>\n",
       "      <td>96</td>\n",
       "      <td>90.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Knights Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Macauley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>96</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Ponzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>La Brûlade</td>\n",
       "      <td>95</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Provence</td>\n",
       "      <td>Bandol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provence red blend</td>\n",
       "      <td>Domaine de la Bégude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 country                                        description  \\\n",
       "0           0      US  This tremendous 100% varietal wine hails from ...   \n",
       "1           1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
       "2           2      US  Mac Watson honors the memory of a wine once ma...   \n",
       "3           3      US  This spent 20 months in 30% new French oak, an...   \n",
       "4           4  France  This is the top wine from La Bégude, named aft...   \n",
       "\n",
       "                            designation  points  price        province  \\\n",
       "0                     Martha's Vineyard      96  235.0      California   \n",
       "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
       "2         Special Selected Late Harvest      96   90.0      California   \n",
       "3                               Reserve      96   65.0          Oregon   \n",
       "4                            La Brûlade      95   66.0        Provence   \n",
       "\n",
       "            region_1           region_2             variety  \\\n",
       "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
       "1               Toro                NaN       Tinta de Toro   \n",
       "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
       "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
       "4             Bandol                NaN  Provence red blend   \n",
       "\n",
       "                    winery  \n",
       "0                    Heitz  \n",
       "1  Bodega Carmen Rodríguez  \n",
       "2                 Macauley  \n",
       "3                    Ponzi  \n",
       "4     Domaine de la Bégude  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# import data \n",
    "wine_data = pd.read_csv(\"data/winemag-data_first150k.csv\", encoding = 'utf8')\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any rows where we have no description\n",
    "wine_data = wine_data[pd.notnull(wine_data['description'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few quick initial thoughts about Computational Linguistics:\n",
    "* computers know nothing about words\n",
    "  * as humans we know that \"US\", \"USA\", \"us\", \"usa\", and \"u.s.a.\" are all referencing the same place, but computers can only compare things that are identical\n",
    "  * if we want to be able to compare words, then we have to make the words as uniform as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA and usa are the same word: False\n",
      "u.s.a. and usa are the same word: False\n",
      "Amanda and amanda are the same word: False\n"
     ]
    }
   ],
   "source": [
    "# In case you don't believe me:\n",
    "print (\"USA and usa are the same word:\" , \"USA\"==\"usa\")\n",
    "print (\"u.s.a. and usa are the same word:\" , \"u.s.a.\"==\"usa\")\n",
    "print(\"Amanda and amanda are the same word:\", \"Amanda\"==\"amanda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "\n",
    "When working with text data, the goal is to process (remove, filter, and combine) the text in such a way that informative text is preserve and munged into a form that models can better understand.  After looking at our raw text, we know that there are a number of textual attributes that we will need to address before we can ultimately represent our text as quantified features. \n",
    "\n",
    "A common first step is to handle [string encoding](http://kunststube.net/encoding/) and formatting issues.  Often it is easy to address the character encoding and mixed capitalization using Python's built-in functions. For our wine example, we will convert everything to UTF-8 encoding and convert all letters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  This tremendous 100% varietal wine hails from Oakville and was aged over three years in oak. Juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background. Balanced and rewarding from start to finish, it has years ahead of it to develop further nuance. Enjoy 2022–2030.\n",
      "Lower-cased:  this tremendous 100% varietal wine hails from oakville and was aged over three years in oak. juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background. balanced and rewarding from start to finish, it has years ahead of it to develop further nuance. enjoy 2022–2030.\n"
     ]
    }
   ],
   "source": [
    "# for simplicity we can look at what this does for a single row:\n",
    "\n",
    "wine1 = wine_data.description.iloc[0]\n",
    "\n",
    "print (\"Original: \", wine1)\n",
    "print(\"Lower-cased: \", wine1.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "In order to process text, it must be deconstructed into its constituent elements through a process termed *tokenization*. Often, the *tokens* yielded from this process are simply individual words in a document.  In certain cases, it can be useful to tokenize stranger objects like emoji or parts of html (or other code).\n",
    "\n",
    "A simplistic way to tokenize text relies on white space, such as in <code>nltk.tokenize.WhitespaceTokenizer</code>. Relying on white space, however, does not take *punctuation* into account, and depending on this some tokens will include punctuation  and will require further preprocessing (e.g. 'account,'). Depending on your data, the punctuation may provide meaningful information, so you will want to think about whether it should be preserved or if it can be removed.\n",
    "\n",
    "Tokenization is particularly challenging in the biomedical field, where many phrases contain substantial punctuation (parentheses, hyphens, etc.) that can't necessarily be ignored. Additionally, negation detection can be critical in this context which can provide an additional preprocessing challenge.\n",
    "\n",
    "NLTK contains many built-in modules for tokenization, such as <code>nltk.tokenize.WhitespaceTokenizer</code> and <code>nltk.tokenize.RegexpTokenizer</code>. It surprisingly also has a module specifically for deal with Twitter data, <code>nltk.tokenize.casual.TweetTokenizer</code> which just has a few features related to handling twitter handles.\n",
    "\n",
    "SpaCy also has built in modules to deal with tokenization. Below we'll look at a few different kinds of tokenizers. \n",
    "\n",
    "See also:\n",
    "\n",
    "[The Art of Tokenization](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)<br>\n",
    "[Negation's Not Solved: Generalizability Versus Optimizability in Clinical Natural Language Processing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4231086/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace Tokenizer\n",
    "One possible method for tokenizing. However, this particular tool identifies words by using whitespace. It thus considers punctuation to be part of a word at times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'tremendous', '100%', 'varietal', 'wine', 'hails', 'from', 'oakville', 'and', 'was', 'aged', 'over', 'three', 'years', 'in', 'oak.', 'juicy', 'red-cherry', 'fruit', 'and', 'a', 'compelling', 'hint', 'of', 'caramel', 'greet', 'the', 'palate,', 'framed', 'by', 'elegant,', 'fine', 'tannins', 'and', 'a', 'subtle', 'minty', 'tone', 'in', 'the', 'background.', 'balanced', 'and', 'rewarding', 'from', 'start', 'to', 'finish,', 'it', 'has', 'years', 'ahead', 'of', 'it', 'to', 'develop', 'further', 'nuance.', 'enjoy', '2022–2030.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "ws_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# tokenize example review\n",
    "wine1_ws = nyt_ws_tokens = ws_tokenizer.tokenize(wine1.lower())\n",
    "print(wine1_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Tokenization\n",
    "\n",
    "By applying the regular expression tokenizer we can more highly tune our tokenizer to yield the types of tokens useful for our data.  Here we return a list of word tokens without punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'tremendous', '100', 'varietal', 'wine', 'hails', 'from', 'oakville', 'and', 'was', 'aged', 'over', 'three', 'years', 'in', 'oak', 'juicy', 'red', 'cherry', 'fruit', 'and', 'a', 'compelling', 'hint', 'of', 'caramel', 'greet', 'the', 'palate', 'framed', 'by', 'elegant', 'fine', 'tannins', 'and', 'a', 'subtle', 'minty', 'tone', 'in', 'the', 'background', 'balanced', 'and', 'rewarding', 'from', 'start', 'to', 'finish', 'it', 'has', 'years', 'ahead', 'of', 'it', 'to', 'develop', 'further', 'nuance', 'enjoy', '2022', '2030']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# tokenize example review\n",
    "wine1_re = re_tokenizer.tokenize(wine1.lower())\n",
    "print(wine1_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "SpaCy is a kind of cool package that can tokenize words, lemmatize, find named entities, etc. So, you can imagine it as an alternative to NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tremendous 100% varietal wine hails from oakville and was aged over three years in oak. juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background. balanced and rewarding from start to finish, it has years ahead of it to develop further nuance. enjoy 2022–2030.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "print(nlp(wine1.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[this, tremendous, 100, %, varietal, wine, hails, from, oakville, and, was, aged, over, three, years, in, oak, ., juicy, red, -, cherry, fruit, and, a, compelling, hint, of, caramel, greet, the, palate, ,, framed, by, elegant, ,, fine, tannins, and, a, subtle, minty, tone, in, the, background, ., balanced, and, rewarding, from, start, to, finish, ,, it, has, years, ahead, of, it, to, develop, further, nuance, ., enjoy, 2022–2030, .]\n"
     ]
    }
   ],
   "source": [
    "# you can use spacy to get individual tokens:\n",
    "wine1_tokens = []\n",
    "for token in nlp(wine1.lower()):\n",
    "    wine1_tokens.append(token)\n",
    "    \n",
    "print(wine1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tremendous 100% varietal wine hails from oakville and was aged over three years in oak.\n",
      "juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background.\n",
      "balanced and rewarding from start to finish, it has years ahead of it to develop further nuance.\n",
      "enjoy 2022–2030.\n"
     ]
    }
   ],
   "source": [
    "# you can also get sentences usinf SpaCy\n",
    "for sent in nlp(wine1.lower()).sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final thoughts on tokenization:\n",
    "Which tokenizer you use depends on what you are going to need for your model ultimately. You should think about what is the most appropriate choice. In some senses it it fine to do something like lowercase, and then get individual words without punctuation, but there are other circumstances when this might not be helpful. For example, imagine if you were trying to identify questions in your input - you may want to use something like SpaCy's tokenizer to get the sentences before further processing your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Depending on the application, many words provide little value when building an NLP model. Moreover, they may provide a source of \"distraction\" for models since model capacity is used to understand words with low information content.  Accordingly, these are termed *stop words*. Examples of stop words include pronouns, articles, prepositions and conjunctions, but there are many other words, or non meaningful tokens, that you may wish to remove. \n",
    "<p>Stop words can be determined and handled in many different ways, including:\n",
    "* Using a list of words determined *a priori* - either a standard list from the NLTK package or one modified from such a list based on domain knowledge of a particular subject\n",
    "* Sorting the terms by *collection frequency*(the total number of times each term appears in the document collection), and then to taking the most frequent terms as a stop list based on semantic content.\n",
    "* Using no defined stop list at all, and dealing with text data in a purely statistical manner. In general, search engines do not use stop lists.\n",
    "\n",
    "As you work with your text, you may decide to iterate on this process. When in doubt, it is often a fruitful strategy to try the above bullets in order.  See also: [Stop Words](http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\n",
    "\n",
    "### Stopword Corpus\n",
    "\n",
    "For this example, we will use the english stopword corpus from NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# here you can see the words included in the stop words corpus\n",
    "print (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the stop words and compare to our original list of tokens from our regular expression tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for token in wine1_re:\n",
    "    if token not in stop_words:\n",
    "        cleaned_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before removing stop words: 62\n",
      "Number of tokens after removing stop words: 38\n"
     ]
    }
   ],
   "source": [
    "print ('Number of tokens before removing stop words: %d' % len(wine1_re))\n",
    "print ('Number of tokens after removing stop words: %d' % len(cleaned_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tremendous', '100', 'varietal', 'wine', 'hails', 'oakville', 'aged', 'three', 'years', 'oak', 'juicy', 'red', 'cherry', 'fruit', 'compelling', 'hint', 'caramel', 'greet', 'palate', 'framed', 'elegant', 'fine', 'tannins', 'subtle', 'minty', 'tone', 'background', 'balanced', 'rewarding', 'start', 'finish', 'years', 'ahead', 'develop', 'nuance', 'enjoy', '2022', '2030']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that by removing stop words, we now have less than half the number of tokens as our original list. Taking a peek at the cleaned tokens, we can see that a lot of the information that makes sentences human-readable has been lost, but the key nouns, verbs, adjectives, and adverbs remain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'somewhere', 'either', 'over', 'us', 'there', 'am', 'three', 'enough', 'indeed', 'take', 'such', 'two', 'never', 'whenever', 'you', 'did', 'few', 'if', 'hereupon', 'others', 'so', 'neither', 'wherever', 'their', 'has', 'very', 'hereby', 'former', 'under', 'anything', 'see', 'while', 'any', 'beyond', 'none', 'name', 'towards', 'too', 'next', 'its', 'my', 'whither', 'whom', 'almost', 'nevertheless', 'otherwise', 'this', 'well', 'above', 'seem', 'but', 'together', 'about', 'because', 'most', 'still', 'even', 'bottom', 'below', 'again', 'just', 'are', 'by', 'beforehand', 're', 'everyone', 'against', 'put', 'thereafter', 'up', 'eleven', 'onto', 'serious', 'yourselves', 'call', 'via', 'when', 'throughout', 'only', 'been', 'himself', 'i', 'thereby', 'these', 'however', 'were', 'each', 'his', 'eight', 'me', 'show', 'rather', 'whether', 'though', 'twelve', 'various', 'all', 'those', 'afterwards', 'should', 'yourself', 'hundred', 'in', 'further', 'namely', 'thru', 'seeming', 'being', 'nine', 'they', 'once', 'her', 'themselves', 'through', 'a', 'where', 'becoming', 'our', 'no', 'ten', 'hence', 'that', 'between', 'who', 'why', 'alone', 'nothing', 'can', 'anyhow', 'around', 'say', 'him', 'moreover', 'than', 'also', 'done', 'thereupon', 'within', 'please', 'per', 'thus', 'top', 'whose', 'your', 'do', 'often', 'something', 'some', 'nor', 'many', 'then', 'have', 'was', 'became', 'get', 'noone', 'regarding', 'third', 'during', 'whoever', 'formerly', 'across', 'go', 'one', 'whereby', 'amount', 'else', 'full', 'it', 'mine', 'out', 'thence', 'sometime', 'everywhere', 'hers', 'them', 'until', 'myself', 'sometimes', 'off', 'side', 'for', 'ever', 'now', 'herself', 'is', 'fifteen', 'first', 'due', 'from', 'of', 'back', 'mostly', 'seemed', 'sixty', 'cannot', 'another', 'the', 'whereupon', 'she', 'on', 'every', 'down', 'quite', 'itself', 'used', 'here', 'he', 'we', 'would', 'after', 'besides', 'perhaps', 'both', 'become', 'keep', 'always', 'herein', 'own', 'along', 'doing', 'someone', 'much', 'latterly', 'ourselves', 'anywhere', 'somehow', 'therein', 'before', 'to', 'several', 'latter', 'unless', 'really', 'more', 'twenty', 'although', 'and', 'anyone', 'at', 'fifty', 'be', 'beside', 'does', 'other', 'same', 'becomes', 'might', 'how', 'whereafter', 'anyway', 'hereafter', 'everything', 'last', 'nowhere', 'using', 'without', 'nobody', 'yet', 'what', 'less', 'give', 'will', 'since', 'forty', 'part', 'yours', 'whence', 'with', 'therefore', 'ca', 'already', 'seems', 'as', 'empty', 'which', 'wherein', 'behind', 'five', 'may', 'whereas', 'amongst', 'least', 'make', 'not', 'six', 'whole', 'toward', 'into', 'four', 'move', 'had', 'among', 'made', 'upon', 'must', 'or', 'ours', 'meanwhile', 'except', 'could', 'elsewhere', 'whatever', 'an', 'front'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS) # <- set of Spacy's default stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to add your own personal stop words\n",
    "STOP_WORDS.add('.')\n",
    "STOP_WORDS.add('%')\n",
    "STOP_WORDS.add(',')\n",
    "STOP_WORDS.add('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** it seems like you have to lemmatize and remove stopwords in the same step..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final thoughts on tokenization\n",
    "You may notice from looking at this sample, however, that a potentially meaningful word has been removed: 'not'. This stopword corpus includes the words 'no', 'nor', and 'not' and so by removing these words we have removed negation. You can set these stopword lists to avoid removing some words, and you can add words to the stop word lists (if for example you really thought that you would never need to care about a particular word, e.g., you didn't care about the use of the word *flamingo* you could add it to the stop word list and it would automatically be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "\n",
    "The overarching goal of stemming and lemmatization is to reduce differential forms of a word to a common base form. By performing stemming and lemmitzation, the count occurrences of words are can be very informative when further processing the data (such as the vectorization, see below). \n",
    "\n",
    "In deciding how to reduce the differential forms of words, you will want to consider how much information you will need to retain for your application. For instance, in many cases markers of tense and plurality are not informative, and so removing these markers will allow you to reduce the number of features.  In other cases, retaining these variations results in better understanding of the underlying content. \n",
    "\n",
    "**Stemming** is the process of representing the word as its root word while removing inflection. For example, the stem of the word 'explained' is 'explain'. By passing this word through a stemming function you would remove the tense inflection. There are multiple approaches to stemming: [Porter stemming](http://snowball.tartarus.org/algorithms/porter/stemmer.html), [Porter2 (snowball) stemming](http://snowball.tartarus.org/algorithms/english/stemmer.html), and [Lancaster stemming](http://www.nltk.org/_modules/nltk/stem/lancaster.html). You can read more in depth about these approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stem of \"explanation\": explan\n",
      "Porter2 (Snowball) Stem of \"explanation\": explan\n",
      "Lancaster Stem of \"explanation\": expl\n"
     ]
    }
   ],
   "source": [
    "print ('Porter Stem of \"explanation\": %s' % porter.stem('explanation'))\n",
    "print ('Porter2 (Snowball) Stem of \"explanation\": %s' %snowball.stem('explanation'))\n",
    "print ('Lancaster Stem of \"explanation\": %s' %lancaster.stem('explanation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While <b><em>stemming</em></b> is a heuristic process that selectively removes the end of words, <b><em>lemmatization</em></b> is a more sophisticated process that can account for variables such as part-of-speech, meaning, and context within a document or neighboring sentences.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print (lemmatizer.lemmatize('explanation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, lemmatization retains a bit more information than stemming. Within stemming, the Lancaster method is more aggressive than Porter and Snowball. Remember that this step allows us to reduce words to a common base form so that we can reduce our feature space and perform counting of occurrences. It will depend on your data and your application as to how much information you need to retain.\n",
    "\n",
    "As a good starting point, see also: [Stemming and lemmatization](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "### Stemming vs Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = []\n",
    "lemmatized_tokens = []\n",
    "\n",
    "for token in cleaned_tokens:\n",
    "    stemmed_tokens.append(snowball.stem(token))\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare: stemmed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tremend', '100', 'variet', 'wine', 'hail', 'oakvill', 'age', 'three', 'year', 'oak', 'juici', 'red', 'cherri', 'fruit', 'compel', 'hint', 'caramel', 'greet', 'palat', 'frame', 'eleg', 'fine', 'tannin', 'subtl', 'minti', 'tone', 'background', 'balanc', 'reward', 'start', 'finish', 'year', 'ahead', 'develop', 'nuanc', 'enjoy', '2022', '2030']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tremendous', '100', 'varietal', 'wine', 'hail', 'oakville', 'aged', 'three', 'year', 'oak', 'juicy', 'red', 'cherry', 'fruit', 'compelling', 'hint', 'caramel', 'greet', 'palate', 'framed', 'elegant', 'fine', 'tannin', 'subtle', 'minty', 'tone', 'background', 'balanced', 'rewarding', 'start', 'finish', 'year', 'ahead', 'develop', 'nuance', 'enjoy', '2022', '2030']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above, it is clear different strategies for generating tokens might retain different information. Moreover, given the transformations stemming and lemmatization apply there will be a different amount of tokens retained in the overall vocabularity.\n",
    "\n",
    "***Critical thoughts:*** It's best to apply intuition and domain knowledge to get a feel for which strategy(ies) to begin with.  In short, it's usually a good idea to optimize for smaller numbers of unique tokens and greater interpretibility as long as it doesn't disagree with common sense and (sometimes more importantly) overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "You can also lemmatize things using SpaCy (but there is no lemmatization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "lemmatizerS = Lemmatizer()\n",
    "lemmatized_tokens_spacy = []\n",
    "\n",
    "for token in wine1_tokens:\n",
    "    lemma = token.lemma_\n",
    "    if lemma not in STOP_WORDS:\n",
    "        lemmatized_tokens_spacy.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tremendous', '100', 'varietal', 'wine', 'hail', 'oakville', 'age', 'year', 'oak', 'juicy', 'red', 'cherry', 'fruit', 'compelling', 'hint', 'caramel', 'greet', 'palate', 'frame', 'elegant', 'fine', 'tannin', 'subtle', 'minty', 'tone', 'background', 'balance', 'rewarding', 'start', 'finish', '-PRON-', 'year', 'ahead', '-PRON-', 'develop', 'nuance', 'enjoy', '2022–2030']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "Another thing that we might care about is the parts of speech (aka POS) of the various words in our reviews. You might, for example, want to do something like count the number of nouns in a review, or the number of adjectives, etc. By tagging words with their POS, you can build features like this.\n",
    "\n",
    "In case you're wondering, this is the list of NLTK POSs:\n",
    "* CC\tcoordinating conjunction\n",
    "* CD\tcardinal digit\n",
    "* DT\tdeterminer\n",
    "* EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "* FW\tforeign word\n",
    "* IN\tpreposition/subordinating conjunction\n",
    "* JJ\tadjective\t'big'\n",
    "* JJR\tadjective, comparative\t'bigger'\n",
    "* JJS\tadjective, superlative\t'biggest'\n",
    "* LS\tlist marker\t1)\n",
    "* MD\tmodal\tcould, will\n",
    "* NN\tnoun, singular 'desk'\n",
    "* NNS\tnoun plural\t'desks'\n",
    "* NNP\tproper noun, singular\t'Harrison'\n",
    "* NNPS\tproper noun, plural\t'Americans'\n",
    "* PDT\tpredeterminer\t'all the kids'\n",
    "* POS\tpossessive ending\tparent's\n",
    "* PRP\tpersonal pronoun\tI, he, she\n",
    "* PRP\\$\tpossessive pronoun\tmy, his, hers\n",
    "* RB\tadverb\tvery, silently,\n",
    "* RBR\tadverb, comparative\tbetter\n",
    "* RBS\tadverb, superlative\tbest\n",
    "* RP\tparticle\tgive up\n",
    "* TO\tto\tgo 'to' the store.\n",
    "* UH\tinterjection\terrrrrrrrm\n",
    "* VB\tverb, base form\ttake\n",
    "* VBD\tverb, past tense\ttook\n",
    "* VBG\tverb, gerund/present participle\ttaking\n",
    "* VBN\tverb, past participle\ttaken\n",
    "* VBP\tverb, sing. present, non-3d\ttake\n",
    "* VBZ\tverb, 3rd person sing. present\ttakes\n",
    "* WDT\twh-determiner\twhich\n",
    "* WP\twh-pronoun\twho, what\n",
    "* WP\\$\tpossessive wh-pronoun\twhose\n",
    "* WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tremend', 'VB'), ('100', 'CD'), ('variet', 'JJ'), ('wine', 'NN'), ('hail', 'NN'), ('oakvill', 'NN'), ('age', 'NN'), ('three', 'CD'), ('year', 'NN'), ('oak', 'NN'), ('juici', 'NN'), ('red', 'JJ'), ('cherri', 'NN'), ('fruit', 'NN'), ('compel', 'NN'), ('hint', 'NN'), ('caramel', 'NN'), ('greet', 'NN'), ('palat', 'JJ'), ('frame', 'NN'), ('eleg', 'JJ'), ('fine', 'JJ'), ('tannin', 'NN'), ('subtl', 'NN'), ('minti', 'FW'), ('tone', 'NN'), ('background', 'NN'), ('balanc', 'NN'), ('reward', 'JJ'), ('start', 'NN'), ('finish', 'JJ'), ('year', 'NN'), ('ahead', 'RB'), ('develop', 'VB'), ('nuanc', 'NN'), ('enjoy', 'NN'), ('2022', 'CD'), ('2030', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# get the pos for our review\n",
    "pos_tagged_review = pos_tag(stemmed_tokens)\n",
    "    \n",
    "print(pos_tagged_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we wanted to count the number of nouns in this review, we could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 nouns in the first review\n"
     ]
    }
   ],
   "source": [
    "nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "count = 0\n",
    "for word, tag in pos_tagged_review:\n",
    "    if tag in nouns:\n",
    "        count +=1\n",
    "    \n",
    "print(\"There are %d nouns in the first review\" %count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(this, 'DET'), (tremendous, 'ADJ'), (100, 'NUM'), (%, 'NOUN'), (varietal, 'ADJ'), (wine, 'NOUN'), (hails, 'VERB'), (from, 'ADP'), (oakville, 'NOUN'), (and, 'CCONJ'), (was, 'VERB'), (aged, 'VERB'), (over, 'ADP'), (three, 'NUM'), (years, 'NOUN'), (in, 'ADP'), (oak, 'NOUN'), (., 'PUNCT'), (juicy, 'ADJ'), (red, 'ADJ'), (-, 'PUNCT'), (cherry, 'NOUN'), (fruit, 'NOUN'), (and, 'CCONJ'), (a, 'DET'), (compelling, 'ADJ'), (hint, 'NOUN'), (of, 'ADP'), (caramel, 'NOUN'), (greet, 'VERB'), (the, 'DET'), (palate, 'NOUN'), (,, 'PUNCT'), (framed, 'VERB'), (by, 'ADP'), (elegant, 'ADJ'), (,, 'PUNCT'), (fine, 'ADJ'), (tannins, 'NOUN'), (and, 'CCONJ'), (a, 'DET'), (subtle, 'ADJ'), (minty, 'NOUN'), (tone, 'NOUN'), (in, 'ADP'), (the, 'DET'), (background, 'NOUN'), (., 'PUNCT'), (balanced, 'VERB'), (and, 'CCONJ'), (rewarding, 'ADJ'), (from, 'ADP'), (start, 'NOUN'), (to, 'ADP'), (finish, 'NOUN'), (,, 'PUNCT'), (it, 'PRON'), (has, 'VERB'), (years, 'NOUN'), (ahead, 'ADV'), (of, 'ADP'), (it, 'PRON'), (to, 'PART'), (develop, 'VERB'), (further, 'ADJ'), (nuance, 'NOUN'), (., 'PUNCT'), (enjoy, 'VERB'), (2022–2030, 'NUM'), (., 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_spacy = []\n",
    "\n",
    "for token in nlp(wine1.lower()):\n",
    "    pos_tagged_spacy.append((token, token.pos_))\n",
    "\n",
    "print(pos_tagged_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 nouns in the first review\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word, tag in pos_tagged_spacy:\n",
    "    if tag =='NOUN':\n",
    "        count +=1\n",
    "    \n",
    "print(\"There are %d nouns in the first review\" %count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You'll note:*** SpaCy and NLTK differ in how many nouns they think exist in the text. This is something you should think about - about the reliability of your POS tagger. Also note, SpaCy sort of requries you to do all of your preprocessing at once, and I found it difficult to use the POS tagger on lemmatized tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Often in natural language processing we want to represent our text as a quantitative set of features for subsequent analysis. We can refer to this as vectorization. One way to generate features from text is to count the occurrences words. This apporoach is often referred to as a <b>bag of words approach</b>.\n",
    "\n",
    "For the example of our article, we can represent the document as a vector of counts for each token. We can do the same for the other articles, and in the end we would have a set of vectors - with each vector representing an article. These vectors could then be used in the next phase of analysis (e.g. classification, document clustering, ...). \n",
    "\n",
    "When we apply a <b>count vectorizer</b> to our corpus of articles, the output will be a matrix with the number of rows corresponding to the number of articles and the number of columns corresponding to the number of unique tokens across (across articles). You can imagine that if we have many articles in a corpus of varied content, the number of unique tokens could get quite large. Some of our preprocessing steps address this issue. In particular, the stemming/lemmatization step reduces the number of unique versions of a word that appear in the corpus. Additionally it is possible to reduce the number of features by removing words that appear least frequently, or by removing words that are common to each article and therefore may not be informative for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorization of Article\n",
    "\n",
    "For this example we will use the stemmed tokens from our article. We will need to join the tokens together to represent one article.\n",
    "\n",
    "Check out the documentation for [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in scikit-learn. You will see that there are a number of parameters that you can specify - including the maximum number of features. Depending on your data, you may choose to restrict the number of features by removing words that appear with least frequency (and this number may be set by cross-validation).\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# stem our example article\n",
    "stemmed_review = ' '.join(wd for wd in stemmed_tokens)\n",
    "\n",
    "# performe a count-based vectorization of the document\n",
    "review_vect = vectorizer.fit_transform([stemmed_review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, we can see that the five most frequently occuring words in this review, are __year, tremend, 100, variet,__ and __wine__: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 words for the first reviewed wine: [('year', 2), ('tremend', 1), ('100', 1), ('variet', 1), ('wine', 1)]\n"
     ]
    }
   ],
   "source": [
    "freqs = [(word, review_vect.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "print (\"top 5 words for the first reviewed wine:\", sorted (freqs, key = lambda x: -x[1])[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can imagine that we could apply this count vectorizer to all of our articles. We could then use the word count vectors in a number of subsequent analyses (e.g. exploring the topics appearing across the corpus).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (tf-idf) Vectorization\n",
    "\n",
    "We have mentioned that you may want to limit the number of features in your vector, and that one way to do this would be to only take the tokens that occur most frequently. Imagine again the above example of trying to differentiate between supporting and opposing documents in a political context. If the documents are all related to the same political initiative, then very likely there will be words related to the intitiative that appear in both documents and thus have high frequency counts. If we cap the number of features by frequency, these words would likely be included, but will they be the most informative when trying to differentiate documents?\n",
    "\n",
    "\n",
    "For many such cases we may want to use a vectorization approach called **term frequency - inverse document frequency (tf-idf)**. [Tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) allows us to weight words by their importance by considering how often a word appears in a given document and throughout the corpus. That is, if a word occurs frequently in a (preprocessed) document it should be important, yet if it also occurs frequently accross many documents it is less informative and differentiating.\n",
    "\n",
    "In our example, the name of the inititative would likely appear numerous times in each document for both opposing and supporting positions. Because the name occurs across all documents, this word would be down weighted in importance. For a more in depth read, these posts go into a bit more depth about text vectorization: [tf-idf part 1](http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/) and [tf-idf part 2](http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "To utilize tf-idf, we will add in additional articles from our dataset. We will need to preprocess the text from these articles and then we can use [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) on our stemmed tokens.\n",
    "\n",
    "To perform tf-idf tranformations, we first need occurence vectors for all our articles using (like the above) count vectorizer.  From here, we could use scikit-learn's [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) to transform our matrix into a tf-idf matrix.\n",
    "\n",
    "For a more complete example, consider a preprocessing pipeline where we first tokenize using regexp, remove standard stop words, perform stemming, and finally convert to tf-idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing review text...\n",
      "preprocessed content for 150930 articles\n",
      "(150930, 21503)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_review_content(text_df):\n",
    "    \"\"\"\n",
    "    Simple preprocessing pipeline which uses RegExp, sets basic token requirements, and removes stop words.\n",
    "    \"\"\"\n",
    "    print ('preprocessing review text...')\n",
    "\n",
    "    # tokenizer, stops, and stemmer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))  # can add more stop words to this set\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # process reviews\n",
    "    review_list = []\n",
    "    for row, review in enumerate(text_df['description']):\n",
    "        cleaned_tokens = []\n",
    "        tokens = tokenizer.tokenize(review.lower())\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                if len(token) > 0 and len(token) < 20: # removes non words\n",
    "                    if not token[0].isdigit() and not token[-1].isdigit(): # removes numbers\n",
    "                        stemmed_tokens = stemmer.stem(token)\n",
    "                        cleaned_tokens.append(stemmed_tokens)\n",
    "        # add process article\n",
    "        review_list.append(' '.join(wd for wd in cleaned_tokens))\n",
    "\n",
    "    # echo results and return\n",
    "    print ('preprocessed content for %d articles' % len(review_list))\n",
    "    return review_list\n",
    "\n",
    "# process articles\n",
    "processed_review_list = preprocess_review_content(wine_data)\n",
    "\n",
    "# vectorize the articles and compute count matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "tfidf_article_matrix = tf_vectorizer.fit_transform(processed_review_list)\n",
    "\n",
    "print (tfidf_article_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that after applying the tf-idf vectorizers to our sample of 150K reviews, we have a sparse matrix with 150K rows (each corresponding to a review) and 21,503 columns (each corresponding to a stemmed token. Depending on our application, we may choose to restrict the number of features (corresponding to the number of columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams v. Bigrams v. Ngrams\n",
    "\n",
    "When we decided to tokenize our corpus above, we decided to treat each word as a token.  A collection of text represented by single words only is a **unigram model of the data**.  This representation can often be surprisingly powerful, because the presence of single words can be hugely informative. \n",
    "\n",
    "However, when dealing with natural language we often want to incorporate structure that is present - grammer, syntactic meaning, and tone.  The **downside of unigrams is that it ignores the ordering of words**, as the order of the token counts is not captured.  The simplest model that captures ordering and structure is one that treats neighboring word pairs as tokens, this is called a **bigram** model.  \n",
    "\n",
    "As an example consider a document that has the words \"good\", \"bad\", and \"project\" in its corpus (with relatively similar count frequencies).  From unigrams alone, its not possible to tell whether the project is good or bad, because those adjectives could appear next to the subject \"project\" or in completely unrelated sentences.  With bigrams, we might then see the token \"good project\" appearing frequently and we would now know significantly more what the document is about.\n",
    "\n",
    "Choosing pairs of words (bigrams) is just the simplest choice we can make. We can generalize this allow tokens of N numbers of words, these are called **Ngrams**.  When N=3 we refer to tokens as trigrams, but for higher values of N we do not typically assign a unique name. \n",
    "\n",
    "***Best practices:*** Generally speaking most NLP models want to have unigrams present.  Very commonly bigrams are important and are also used to build high quality models.  Typically higher order Ngrams are less common, as the number of features (and computational requirements) increase rapidly and yield diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Another vectorization option is to use a word embedding model to generate vector representations of words. Word embedding models create non-linear representations of words, which account for the context and neighboring language surround a word.  A common model(which has many pretrained libraries) is [Word2Vec](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html).\n",
    "\n",
    "Word embedding models have gained lots of popularity, as they are able to capture syntactic meaning quite well.  However, good vector representations are only appropriate for the corpus they are trained on and often they will not generate good models for corpuses which are significantly different.  For instance, a Word2Vec model trained on literature may not be appropriate for Twitter or StackOverflow text data.  The alternative in these cases is to retrain the model on the correct data, but this is hard - it requires lots of data, choices, and computation to generate good representations.  As a first approach, it's probably best to start with Ngrams using counts or tf-idf weightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300-SLIM.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words To Paragraphs: Vector Averaging\n",
    "One challenge with the wine dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "import re\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to see if we can train a model to predict something (for simplicity's sake, let's say I want to predict the wine score) we need to split our data into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>US</td>\n",
       "      <td>This standout Rocks District wine brings earth...</td>\n",
       "      <td>The Funk Estate</td>\n",
       "      <td>94</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Walla Walla Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>Saviah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>535</td>\n",
       "      <td>France</td>\n",
       "      <td>The wine is pure minerality, tightly woven int...</td>\n",
       "      <td>Mandelberg Grand Cru</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>Jean Becker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>695</td>\n",
       "      <td>US</td>\n",
       "      <td>Cherry-rhubarb juice, shiitake mushrooms, sage...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>30.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sta. Rita Hills</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Fess Parker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>US</td>\n",
       "      <td>This youthful wine is shyer on the nose than i...</td>\n",
       "      <td>Montecillo Vineyard</td>\n",
       "      <td>90</td>\n",
       "      <td>42.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>Ordaz Family Wines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>836</td>\n",
       "      <td>US</td>\n",
       "      <td>Aromas of gingerbread cookies with a black-che...</td>\n",
       "      <td>Bailey Ranch Riserva</td>\n",
       "      <td>90</td>\n",
       "      <td>40.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Adelaida District</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>Bella Luna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 country                                        description  \\\n",
       "29           29      US  This standout Rocks District wine brings earth...   \n",
       "535         535  France  The wine is pure minerality, tightly woven int...   \n",
       "695         695      US  Cherry-rhubarb juice, shiitake mushrooms, sage...   \n",
       "557         557      US  This youthful wine is shyer on the nose than i...   \n",
       "836         836      US  Aromas of gingerbread cookies with a black-che...   \n",
       "\n",
       "              designation  points  price    province                 region_1  \\\n",
       "29        The Funk Estate      94   60.0  Washington  Walla Walla Valley (WA)   \n",
       "535  Mandelberg Grand Cru      92    NaN      Alsace                   Alsace   \n",
       "695                   NaN      90   30.0  California          Sta. Rita Hills   \n",
       "557   Montecillo Vineyard      90   42.0  California            Sonoma Valley   \n",
       "836  Bailey Ranch Riserva      90   40.0  California        Adelaida District   \n",
       "\n",
       "            region_2     variety              winery  \n",
       "29   Columbia Valley       Syrah              Saviah  \n",
       "535              NaN    Riesling         Jean Becker  \n",
       "695    Central Coast  Pinot Noir         Fess Parker  \n",
       "557           Sonoma   Zinfandel  Ordaz Family Wines  \n",
       "836    Central Coast   Zinfandel          Bella Luna  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# make smaller df\n",
    "wine_data = wine_data[pd.notnull(wine_data['points'])]\n",
    "df = wine_data\n",
    "df = df.iloc[:1000,]\n",
    "\n",
    "\n",
    "# split the data \n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = set( stopwords.words(\"english\") )\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def review_to_wordlist( raw_review, remove_stopwords = False ):\n",
    "    \n",
    "    words = re.sub(\"[^a-zA-Z]\",\" \",raw_review).lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "num_features = 300 # this is the length of our word vectors\n",
    "clean_train_reviews = []\n",
    "for review in train[\"description\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print (\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"description\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.1903963278101209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/insight/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "forest = forest.fit( trainDataVecs, train[\"points\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "print(\"F1 score: \", f1_score(test[\"points\"], result, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "Turns out that the F1 score for our random forest isn't great. We could tune this better, or we could resolve that maybe other features might be better at identifying our wine scores (e.g., are there predictive keywords? does the sentiment of the review better reflect scores?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "If you want to know how positive / neutral / negative a review is, you can try passing it through a sentiment analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentiment_scores(sentence):\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(snt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tremendous 100% varietal wine hails from Oakville and was aged over three years in oak. Juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background. Balanced and rewarding from start to finish, it has years ahead of it to develop further nuance. Enjoy 2022–2030. {'neg': 0.0, 'neu': 0.768, 'pos': 0.232, 'compound': 0.9287}\n"
     ]
    }
   ],
   "source": [
    "print_sentiment_scores(wine1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to compute the various scores for all wines in our table and use those scores as features in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "For some tasks you might be interested in finding the named entities in the text. This is similar to POS tagging, only instead of getting the parts of speech, you get what kind of thing a word is (e.g., 'PERSON', 'PLACE'). This is particularly useful if you want to provide some sort of summary, or if you want to know something about what was in general talked about.\n",
    "\n",
    "I'm using SpaCy because it was easy, but not necessarily the best. For more on the types see: [SpaCy's website](https://spacy.io/usage/linguistic-features#section-named-entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% PERCENT\n",
      "Oakville GPE\n",
      "aged over three years DATE\n",
      "2022–2030 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)\n",
    "doc = nlp(wine1)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things that I didn't get around to discussing:\n",
    "Other ways to vectorize:\n",
    "* Facebook's FastText: similar to Word2Vec except that instead of building vectors for words it builds vectors for sub-word embeddings\n",
    "  * this means that it builds vectors for the word \"apple\" such as: \"a\", \"pple\", \"ap\", \"ple\", \"app\", \"le\", \"appl\", \"e\" and \"apple\"\n",
    "  * the vectors for things such as \"app\" and \"le\" and \"app\" and \"el\" are probably more similar than \"app\" and \"fe\" making it more robust to spelling errors\n",
    "  * it can also handle made up words and infrequent words a bit better than word2vec\n",
    "* doc2vec, sent2vec, etc: all variations on word2vec\n",
    "  * word2vec is just a means of matching words to vectors\n",
    "  * sent2vec would mean matching setences to vectors, etc\n",
    "* Topic Modelling:\n",
    "  * What are the most important topics in the document?\n",
    "  * LSA, LDA, etc (hopefully covered by someone at a later point?)\n",
    "* Cosine difference:\n",
    "  * a measure that can be used to find the most similar reviews to reviews we have vectors for - could choose to give the same wine score as the review with the most similar vector\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
